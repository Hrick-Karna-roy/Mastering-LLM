# Simple Transformer Implementation

This repository contains a basic implementation of a Transformer model with attention mechanisms, written in PyTorch. The code is designed for educational purposes and provides a simplified understanding of how Transformers work.

## Key Features

- **Vanilla Transformer:** A simple implementation of the Transformer architecture, including the encoder and decoder modules.
- **Attention Mechanism:** Demonstrates the core attention mechanism used in Transformers to weigh the importance of different input elements.
- **Educational Focus:** The code is well-commented and easy to follow, making it suitable for learning about Transformer fundamentals.

## Getting Started

1. **Prerequisites:** Make sure you have PyTorch installed.